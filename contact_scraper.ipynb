{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------scrapes washington state bar members contact info ------------------------------\n",
    "# create an empty list to store the LegalProfile URLs\n",
    "legal_profile_urls = []\n",
    "\n",
    "# specify the range of pages to scrape there are 1734 pages\n",
    "start_page = 1\n",
    "end_page = 3\n",
    "\n",
    "# loop through the pages in the specified range\n",
    "for page_number in range(start_page, end_page+1):\n",
    "    # make a request to the current page\n",
    "    url = f'https://www.mywsba.org/PersonifyEbusiness/LegalDirectory.aspx?ShowSearchResults=TRUE&EligibleToPractice=Y&Country=USA&Page={page_number}'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # create a BeautifulSoup object from the response content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # find all the <tr> elements with class \"grid-row\"\n",
    "    rows = soup.find_all('tr', {'class': 'grid-row'})\n",
    "    \n",
    "    # loop through the rows and extract the LegalProfile URL from the onclick attribute\n",
    "    for row in rows:\n",
    "        onclick = row.get('onclick')\n",
    "        if onclick:\n",
    "            url = onclick.split(\"'\")[1]\n",
    "            legal_profile_urls.append(f\"https://www.mywsba.org/PersonifyEbusiness/\"+str(url))\n",
    "\n",
    "    print(page_number)\n",
    "\n",
    "# initialize empty list to store scraped data\n",
    "data = []\n",
    "\n",
    "# loop through each URL and scrape the data\n",
    "for url in legal_profile_urls:\n",
    "    # make a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    # parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # find the relevant elements by ID and extract the text\n",
    "    member_no = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblMemberNo'}).text\n",
    "    license_type = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblLicenseType'}).text\n",
    "    eligible_to_practice = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblEligibleToPractice'}).text\n",
    "    status = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblStatus'}).text\n",
    "    wa_admit_date = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblWaAdmitDate'}).text\n",
    "    email = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblEmail'}).text\n",
    "    member_name = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblMemberName'}).text\n",
    "    address = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblAddress'}).text\n",
    "    phone = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblPhone'}).text\n",
    "    area_of_practice = soup.find('span', {'id': 'dnn_ctr2977_DNNWebControlContainer_ctl00_lblPracticeAreas'}).text\n",
    "\n",
    "    # append the data to the list\n",
    "    data.append([member_name, member_no, license_type, eligible_to_practice, status, wa_admit_date, email, address, phone, area_of_practice])\n",
    "    print(url)\n",
    "    \n",
    "# write the data to a CSV file\n",
    "#with open('Washington_Bar_Association.csv', 'w', newline='') as file:\n",
    "with open('test.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Full_Name','Member_No.', 'License_Type', 'Eligible_to_Practice', 'Status', 'WSBA_Admit_Date', 'Email','address','phone','legal_speciality'])\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------add a state registed column to the current Washington csv's-----\n",
    "\n",
    "# Read the CSV file into a pandas dataframe\n",
    "df = pd.read_csv('Washington_Bar_Association.csv')\n",
    "\n",
    "# Add a new column called 'state_registered' and set all values to 'WA'\n",
    "df['state_registered'] = 'WA'\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df.to_csv('Washington_Bar_Association1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read to csv\n",
    "df = pd.read_csv('Washington_Bar_Association1.csv')\n",
    "\n",
    "#rename columns\n",
    "df = df.rename(columns={\"Full_Name\": \"Name\", \"legal_speciality\": \"legal_specialty\"}, errors=\"raise\")\n",
    "\n",
    "# formate birthday to a datetime\n",
    "# convert the WSBA_Admit_Date column to a string\n",
    "df['WSBA_Admit_Date'] = df['WSBA_Admit_Date'].astype(str)\n",
    "# split the date string into month, day, and year\n",
    "date_parts = df['WSBA_Admit_Date'].str.split('/', expand=True)\n",
    "month = date_parts[0]\n",
    "# add a '0' to the month if necessary\n",
    "month = month.apply(lambda x: x.zfill(2))\n",
    "# combine the month, day, and year back into a date string\n",
    "df['WSBA_Admit_Date'] = month + '/' + date_parts[1] + '/' + date_parts[2]\n",
    "# convert the \"WSBA_Admit_Date\" column to pandas datetime objects\n",
    "df['WSBA_Admit_Date'] = pd.to_datetime(df['WSBA_Admit_Date'], format='%m/%d/%Y')\n",
    "# reformat the \"WSBA_Admit_Date\" column to \"dd-mm-yyyy\" format\n",
    "df['WSBA_Admit_Date'] = df['WSBA_Admit_Date'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "# drop rows with no email\n",
    "df = df.dropna(subset=['Email'])\n",
    "\n",
    "#replace \"\" with '' in the name column to prevent sendy upload error\n",
    "df['Name'] = df['Name'].str.replace('\"', \"'\")\n",
    "\n",
    "#rename columns\n",
    "df = df.rename(columns={\"Full_Name\": \"Name\", \"WSBA_Admit_Date\": \"bar_admittance_date\",\"address\": \"mailing_address\" })\n",
    "df = df[['Name',\"Email\", \"Status\",\"bar_admittance_date\", \"mailing_address\", \"phone\", \"state_registered\",\"License_Type\",\"Member_No.\", \"legal_specialty\"]]\n",
    "\n",
    "display(df)\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df.to_csv('Washington_Bar_Association1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split washington_contact_csv into chunks to be uploaded into sendy\n",
    "#split csv into chunks of 970 rows\n",
    "chunk_size = 5000\n",
    "\n",
    "# read the CSV file into a pandas dataframe\n",
    "df = pd.read_csv('Washington_Bar_Association1.csv')\n",
    "\n",
    "# split the dataframe into chunks of 970 rows\n",
    "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# save each chunk to a separate CSV file\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv(f'./washington_contacts/Washington_Bar_Association_chunk_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------scrapes idaho state bar members contact info-----------------------\n",
    "# Launch a new Chrome browser instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the website\n",
    "url = 'https://apps.isb.idaho.gov/licensing/attorney_roster.cfm'\n",
    "driver.get(url)\n",
    "\n",
    "# Find the button using its class name\n",
    "button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'input[type=\"submit\"]')))\n",
    "\n",
    "# Click the button\n",
    "button.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Get the page source after clicking the button\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "#list of idaho lawyers profile urls\n",
    "lawyer_profile_urls = []\n",
    "\n",
    "# Scrape all hrefs from the table using Beautiful Soup\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "table = soup.find('table', {'class': 'table table-striped dataTable no-footer table-hover'})\n",
    "links = table.find_all('a')\n",
    "count = 0\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    if href:\n",
    "        lawyer_profile_urls.append('https://apps.isb.idaho.gov/licensing/' + str(href))\n",
    "    count += 1\n",
    "    print(f\"added {count} lawyers profile urls to lawyer profile url list\")\n",
    "\n",
    "contact_info = []\n",
    "\n",
    "count1 = 0\n",
    "for url in lawyer_profile_urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # full_name = soup.find('class', {'id': 'panel-title'}).text\n",
    "    try:    \n",
    "        data = {}\n",
    "\n",
    "        # Extract the full name from the h4 attribute with class 'panel-title'\n",
    "        full_name = soup.find('h4', {'class': 'panel-title'}).text.strip()\n",
    "        data['full_name'] = full_name\n",
    "\n",
    "        for dt in soup.find_all('dt'):\n",
    "            key = dt.text.strip()\n",
    "            if key == 'Mailing Address':\n",
    "            # If the dt text is 'Mailing Address', use the text of the next 2 siblings\n",
    "                value = dt.find_next_sibling('dd').text.strip() + \", \" + dt.find_next_sibling('dd').find_next_sibling('dd').text.strip()\n",
    "            else:\n",
    "                value = dt.find_next_sibling('dd').text.strip()\n",
    "\n",
    "            data[key] = value\n",
    "\n",
    "        contact_info.append(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "    count1 += 1\n",
    "    print(f\"added {count1} lawyers contact information to contact info list\")\n",
    "\n",
    "\n",
    "with open('data.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(contact_info[0].keys())  # write header row\n",
    "    for row in contact_info:\n",
    "        writer.writerow(row.values())\n",
    "\n",
    "    print('sucessfully wrote Idaho lawyer contact info to csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------add a 'state_registed column to the current Idaho csv's-----\n",
    "\n",
    "# Read the CSV file into a pandas dataframe\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Add a new column called 'state_registered' and set all values to 'WA'\n",
    "df['state_registered'] = 'ID'\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "df.to_csv('Idaho_Bar_Association1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read to csv\n",
    "df = pd.read_csv('Idaho_Bar_Association1.csv')\n",
    "\n",
    "#rename columns\n",
    "df = df.rename(columns={\"full_name\": \"Name\", \"Admittance Date\": \"bar_admittance_date\", \"Firm\": \"firm\",\"Mailing Address\": \"mailing_address\", \"Phone\": \"phone\", \"Phone Ext\": \"phone_ext\", \"Bar Email Address\": \"Email\", \"Website Address\": \"website_address\", \"Court eService Email\": \"court_eService_email\"}, errors=\"raise\")\n",
    "\n",
    "#formate birthday to a datetime\n",
    "# convert the \"bar_admittance_date\" column to pandas datetime objects\n",
    "df['bar_admittance_date'] = pd.to_datetime(df['bar_admittance_date'], format='%m/%d/%Y')\n",
    "\n",
    "# reformat the \"bar_admittance_date\" column to \"dd-mm-yyyy\" format\n",
    "df['bar_admittance_date'] = df['bar_admittance_date'].dt.strftime('%d-%m-%Y')\n",
    "df = df[['Name',\"Email\",\"Status\",\"bar_admittance_date\",'firm','mailing_address','phone','website_address','court_eService_email','state_registered','phone_ext']]\n",
    "\n",
    "# Write the updated dataframe to a new CSV file\n",
    "# df.to_csv('Idaho_Bar_Association1.csv', index=False)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change order of columns\n",
    "df = pd.read_csv('Idaho_Bar_Association_contacts.csv') #source file deleted\n",
    "df = df[['Name',\"Email\",\"Status\",\"bar_admittance_date\",'firm','mailing_address','phone','phone_ext','website_address','court_eService_email','state_registered']]\n",
    "df = df.dropna(subset=['Email'])\n",
    "df['Name'] = df['Name'].str.replace('\"', \"'\")\n",
    "df.to_csv('Idaho_Bar_Association_contacts1.csv', index=False)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split csv into chunks of 970 rows to be uploaded into sendy\n",
    "chunk_size = 968\n",
    "\n",
    "# read the CSV file into a pandas dataframe\n",
    "df = pd.read_csv('Idaho_Bar_Association_contacts1.csv')\n",
    "\n",
    "# split the dataframe into chunks of 970 rows\n",
    "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# save each chunk to a separate CSV file\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv(f'./idaho_contacts/Idaho_Bar_Association_chunk_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------scrape state of Mississippi bar directory------------------------------\n",
    "# Launch a new Chrome browser instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the website\n",
    "url = 'https://www.msbar.org/lawyer-directory.aspx?type=7&term=A&response=03AKH6MRH19wTOAKE1f1ufR2dIeNH3AWn3nk0p90DQVE1uNvqGKh7F3cwNrLUkk0wxqcg3p9NYERDDSavMoaorNFMWzNFHzQnQxYrzww9Qa3wpDbeq-rF4STgHT75NuZsq1rR9LsfavuG-4uymLQMxsSfUe3SnXhjr50fmDr9P1AxbTx7BT47Kxy6f93IUrfK9a2c9ptmu6LMKJLZ0jeGoIh4oydnyzpljghoG95Kju6fP6QXox-uYBhP9D4F-6BtkEKTXstXn__em503Hngv7ubN7DwjN5RlQjE7gri9NSyfQLxuRzzCWE55rkfElTs-itIEeku51-t7vABggRK-JE7BFwBzswtPZXD7lMJSpIHnNreFdh560cNP7ABCDghcmo2Fs4cqf_ABYY0U4fxWQU3p9opYFv1pfkxd8Nt88V8EzoRNy4RFjgTc0IpQA4ZtXEG-IckSd7XyFm1a0hPBd1wuQT-mHpPFppVMfsVW_BECn8jSVjgfr7FutCKtbnb-ar3mif20eTWk_yry70LGGhP9wehy_ZRvRDGwlpcb3iSxZMPGPTxHKdeEAKWxa9Gd4H5BQ6RzNlhQyluDUvR0idKlvm_3vO35Dfw'\n",
    "driver.get(url)\n",
    "\n",
    "# Find the button using its class name\n",
    "button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'searchbutton')))\n",
    "\n",
    "# Click the button\n",
    "button.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Get the page source after clicking the button\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Parse the HTML content of the response using Beautiful Soup\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find all the sections with class 'LawyerInformation cf'\n",
    "sections = soup.find_all('section', {'class': 'LawyerInformation cf'})\n",
    "\n",
    "# Initialize a list to store the dictionaries\n",
    "results = []\n",
    "\n",
    "# Loop through each section and extract the label-value pairs\n",
    "for section in sections:\n",
    "    data_dict = {}\n",
    "    label_holders = section.find_all('div', {'class': 'LabelHolder'})\n",
    "    data_holders = section.find_all('div', {'class': 'DataHolder'})\n",
    "    for label_holder, data_holder in zip(label_holders, data_holders):\n",
    "        label = label_holder.get_text(strip=True)\n",
    "        data = data_holder.get_text(strip=True)\n",
    "        data_dict[label] = data\n",
    "    results.append(data_dict)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://isba.reliaguide.com/lawyer/60601-IL-Amy-Richards-138552\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "link = soup.find(\"a\", {\"class\": \"gx-link\"})\n",
    "\n",
    "\n",
    "if link:\n",
    "    href = link.get(\"href\")\n",
    "    tooltip_div = link.find_previous_sibling(\"div\", {\"class\": \"ant-tooltip-inner\"})\n",
    "    full_name_div = tooltip_div.find(\"div\", {\"class\": \"gx-text-center gx-mb-0\"})\n",
    "    full_name = full_name_div.find(\"p\", {\"class\": \"gx-mb-0 gx-text-none gx-fs-xl gx-font-weight-light\"}).text.strip()\n",
    "\n",
    "    print(f\"Link: {href}\")\n",
    "    print(f\"Full name: {full_name}\")\n",
    "\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------currently bypasses recaptcha for maryland state bar but does not scrape due to ToS----------------\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize the WebDriver and navigate to the website\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.msba.org/about/member-directory/?fwp_member_directory_bar_admission_state=maryland\")\n",
    "\n",
    "# Wait for the reCaptcha checkbox to appear and click it\n",
    "wait = WebDriverWait(driver, 10)\n",
    "recaptcha_frame = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"iframe[title='reCAPTCHA']\")))\n",
    "driver.switch_to.frame(recaptcha_frame)\n",
    "recaptcha_checkbox = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.recaptcha-checkbox-border\")))\n",
    "ActionChains(driver).move_to_element(recaptcha_checkbox).click().perform()\n",
    "\n",
    "# Switch back to the main frame and wait for the submit button to appear and click it\n",
    "driver.switch_to.default_content()\n",
    "submit_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button#submitBtn[type='submit']\")))\n",
    "submit_button.click()\n",
    "\n",
    "# Close the WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-----------------wisconsin------------------------\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# create an empty list to store the LegalProfile URLs\n",
    "legal_profile_links = []\n",
    "\n",
    "start_number = 0\n",
    "end_number = 10\n",
    "\n",
    "count = 0\n",
    "# URL of the website to scrape\n",
    "for page_num in range(start_number, end_number+1,10):\n",
    "    url = f'https://www.wisbar.org/Pages/AdvancedLawyerSearch-Updated.aspx?refinementfilters=%27SBW-ProfileState%3a(%22wi%22)%27&state=wi&sourceid=%279f1b1dca-5f9d-406e-9067-6b5597699787%27&querytemplatepropertiesurl=%27spfile%3a%2f%2fwebroot%2fqueryparametertemplate.xml%27&selectproperties=%27path%2csbw-profilelastname%2csbw-profilefullname%2csbw-profilefirstname%2csbw-profilestate%2csbw-profilemiddlename%2csbw-profilecompany%2csbw-profilecity%2csbw-profileprefixname%27&StartRow={page_num}'\n",
    "\n",
    "    # Send GET request to the website and parse the HTML using Beautiful Soup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class SearchResult\n",
    "    search_results = soup.find_all('div', class_='SearchResult')\n",
    "\n",
    "    # Extract href links from each div element and append to a list\n",
    "    for result in search_results:\n",
    "        link = result.find('a')['href']\n",
    "        legal_profile_links.append(link)\n",
    "    count += 10\n",
    "    print(f\"appended {count} contact profiles to legal_profile_links\")\n",
    "\n",
    "print(legal_profile_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up the webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "# Loop over each URL\n",
    "for url in legal_profile_links:\n",
    "    # Open the URL in the webdriver\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Add a delay before clicking the checkbox\n",
    "    time.sleep(random.uniform(2.0, 4.0))\n",
    "\n",
    "    # Find the checkbox with iframe tag and title='reCAPTCHA'\n",
    "    iframe = driver.find_element(By.XPATH, '//iframe[@title=\"reCAPTCHA\"]')\n",
    "    driver.switch_to.frame(iframe)\n",
    "\n",
    "    # Wait for the checkbox to become clickable\n",
    "    checkbox = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//span[@class=\"recaptcha-checkbox goog-inline-block recaptcha-checkbox-unchecked rc-anchor-checkbox\"]')))\n",
    "    \n",
    "    # Add a delay before clicking the checkbox\n",
    "    time.sleep(random.uniform(2.0, 4.0))\n",
    "\n",
    "    # Move the mouse over the checkbox before clicking\n",
    "    action = webdriver.ActionChains(driver)\n",
    "    action.move_to_element(checkbox).perform()\n",
    "\n",
    "    # Click on the checkbox using JavaScript\n",
    "    driver.execute_script(\"arguments[0].click();\", checkbox)\n",
    "\n",
    "    # Add a delay after clicking the checkbox\n",
    "    time.sleep(random.uniform(2.0, 4.0))\n",
    "\n",
    "    # Switch back to the main frame and use BeautifulSoup to scrape the text from the li tag with class=Name2\n",
    "    driver.switch_to.default_content()\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    name_list = []\n",
    "    for name in soup.find_all('li', class_='Name2'):\n",
    "        name_list.append(name.text)\n",
    "\n",
    "    # Print the scraped text\n",
    "    print(\"Here are the names we found on the page:\")\n",
    "    for name in name_list:\n",
    "        print(name)\n",
    "    print()\n",
    "\n",
    "    # Add a delay before moving on to the next URL\n",
    "    time.sleep(random.uniform(2.0, 4.0))\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internshipvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
